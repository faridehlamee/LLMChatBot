# Environment variables for production deployment
# Copy this to .env file for local development
# Set these in your deployment platform

# LLM Provider Selection
LLM_PROVIDER="gemini"  # "ollama" for local, "gemini" for Google Gemini

# Production mode
PRODUCTION=true

# Server settings
HOST=0.0.0.0
PORT=8000

# Ollama settings (for production, you'll need a hosted Ollama service)
OLLAMA_URL=https://your-ollama-service.com
OLLAMA_MODEL=llama3.1

# Google Gemini Configuration (FREE!)
GEMINI_API_KEY="AIzaSyCJhL9UEttHk44V8huARvxCpdz8SaOAMMg"
GEMINI_MODEL="gemini-2.5-flash"

# Knowledge base
KNOWLEDGE_BASE_PATH=knowledge_base.json

# Optional: Add any API keys or secrets here
# OPENAI_API_KEY=your_key_here
# ANTHROPIC_API_KEY=your_key_here
